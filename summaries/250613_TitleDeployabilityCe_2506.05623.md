**一句话概要**  
作者提出了一种基于大语言模型的迭代框架IaCGen，通过部署导向的反馈机制显著提升基础设施即代码模板的生成质量，并建立了首个综合评估部署能力的基准DPIaC-Eval。

**主体**  
当前基础设施即代码（IaC）生成研究存在关键缺陷：主流方法仅关注代码的语法正确性，却忽视了实际部署成功率这一核心指标。这种脱节导致生成的模板看似规范却难以真正投入使用，严重制约了自动化云资源编排的实用性。作者指出，现有大语言模型在首次尝试生成IaC模板时部署成功率不足30%，暴露出自然语言到可执行配置之间的转化鸿沟。

为解决这一问题，研究团队设计了IaCGen框架，其创新性在于引入部署验证驱动的迭代优化机制。该框架通过实时检测生成模板的部署错误，将诊断信息反馈给大语言模型进行多轮修正，形成"生成-验证-优化"的闭环流程。同时，作者构建了包含153个真实场景的DPIaC-Eval基准，首次实现对语法合规性、部署可行性、用户意图匹配度和安全合规性的四位一体评估。

实验结果表明，迭代机制带来质的飞跃：经过25轮优化后，主流大语言模型的部署成功率从初始的26-30%跃升至98%，验证了反馈循环对提升实用性的关键作用。但深层分析也揭示出持续挑战——模型在准确理解用户需求方面仅有25.2%的匹配度，安全规范符合率更是低至8.4%，说明语义理解与领域知识融合仍是待攻克的难题。

**最后一句**  
这项研究不仅为IaC生成设立了新的质量标杆，其迭代优化范式更为其他需要将自然语言转化为可执行规范的领域提供了可借鉴的方法论。