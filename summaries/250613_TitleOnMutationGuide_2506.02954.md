**一句话概要**  
作者提出MUTGEN方法，通过将变异测试反馈直接融入大语言模型（LLM）的提示中，显著提升了单元测试生成在检测软件缺陷方面的有效性，突破了传统代码覆盖率指标的局限性。

**主体**  
当前单元测试生成工具普遍依赖代码覆盖率（如行覆盖、分支覆盖）作为核心指标，但研究发现这些指标与测试套件的缺陷检测能力关联性较弱。例如，某些测试套件虽能达到100%覆盖率，却仅能检测4%的变异体（即人工注入的缺陷）。相比之下，变异分数（mutation score）能更严格地衡量测试有效性，但现有基于LLM的测试生成方法对此关注不足，且缺乏系统性优化策略。

针对这一核心问题，作者设计了MUTGEN框架，其创新性体现在两方面：一是将变异测试结果动态反馈至LLM的提示中，指导模型生成针对性更强的测试用例；二是引入迭代生成机制，通过多轮优化逐步消灭更多变异体。实验选取204个来自两个基准集的被测对象，结果显示MUTGEN的变异分数显著优于传统工具EvoSuite和基础提示策略。例如，在部分案例中，MUTGEN的变异分数达到后者的2-3倍，验证了变异反馈对LLM生成方向的精准引导作用。

进一步分析揭示了LLM生成测试的局限性：某些变异体存活（未被杀死）的原因包括逻辑复杂性超出模型理解范围，或测试断言未能覆盖特定变异操作符触发的边界条件。研究还发现，不同变异操作符（如算术运算符替换、条件边界修改）对生成效果的影响存在显著差异，这为未来优化测试生成提供了重要方向。

**最后一句**  
这项工作不仅为基于LLM的测试生成建立了更可靠的评估范式，其迭代反馈机制也为探索AI在复杂软件验证任务中的边界提供了方法论启示。